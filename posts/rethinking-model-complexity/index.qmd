---
title: "Rethinking Model Complexity in Overparameterized Models"
author: "Kalyaan Rao"
date: "2025-01-04"
categories: [singular learning theory, bayesian inference, model selection]
toc: true
toc-depth: 3
---

## 1. why model complexity is not the number of parameters

model selection criteria often penalize models according to the number of parameters they use. this is reasonable when every parameter direction independently affects predictions. but many modern models violate this assumption: different parameter values can represent the same function. low-rank regression, overcomplete models, and neural networks with symmetries all have redundant coordinates.

a simple thought experiment makes this issue clear. consider two models that represent the same family of functions:

- model a uses a minimal coordinate system
- model b uses an inflated coordinate system with redundant directions

functionally, these models are identical. yet parameter-count penalties will prefer model a, appealing to occam's razor. this mismatch motivates a basic question:

**what does bayesian evidence actually count when parameters are redundant?**

---

## 2. where the bic penalty comes from and where the assumption enters

before diving into formulas, let's build intuition for why curvature matters at all. imagine standing in a valley:

- in some directions the ground slopes steeply upward
- in others it is almost flat

if we only know our location up to measurement noise, steep directions pin us down precisely, while flat directions leave us uncertain. bayesian evidence is, at its core, a calculation of how much volume of plausible parameter space remains after seeing data.

curvature determines how fast this volume shrinks as we collect more data.

we observe a dataset
$$
D_n = \{(x_i, y_i)\}_{i=1}^n
$$
of size $n$. a model is parameterized by $\theta \in \mathbb{R}^d$ with prior distribution $\pi(\theta)$.

the likelihood is
$$
p(D_n \mid \theta) = \prod_{i=1}^n p(y_i \mid x_i, \theta),
$$
and the marginal likelihood (evidence) is
$$
p(D_n) = \int p(D_n \mid \theta)\, \pi(\theta)\, d\theta.
$$

throughout this post, we consider the asymptotic regime where the sample size $n$ grows while the model class is fixed.

geometrically, it is useful to distinguish two kinds of directions in parameter space:

- **curved directions**: small changes in $\theta$ lead to quadratic changes in the loss or log-likelihood
- **flat directions**: changes in $\theta$ leave the model's predictions unchanged (or only change them at higher order)

the central theme is that bayesian evidence is sensitive to curved directions, not the raw number of parameters.

for large sample size $n$, the marginal likelihood is approximated using laplace's method. the key steps are:

- the posterior concentrates near a maximizer $\hat{\theta}$
- near $\hat{\theta}$, the log posterior is approximated by a quadratic
- integrating a gaussian in $d$ dimensions yields a volume factor

writing $H_n$ for the hessian of the negative log posterior at $\hat{\theta}$,
$$
\log p(D_n)
\approx
\log p(D_n \mid \hat{\theta})
- \tfrac{1}{2} \log \det H_n + O(1).
$$

at this point, it is useful to introduce the fisher information. it measures how sensitive the likelihood is to small parameter changes:

- large fisher information means predictions change rapidly
- small fisher information means predictions barely change

in regular models, the fisher information matrix is full rank. mathematically,
$$
H_n \approx n\, I(\theta^\star),
$$
where $I(\theta^\star)$ is the fisher information at a kl minimizer.

if $I(\theta^\star)$ is full rank with dimension $d$,
$$
\log \det H_n \sim d \log n,
$$
leading to
$$
\log p(D_n)
\approx
\log p(D_n \mid \hat{\theta})
- \frac{d}{2} \log n + O(1),
$$
which is the bayesian information criterion (bic).

crucially, this derivation assumes every parameter direction contributes curvature. if some directions are flat, this scaling breaks.

---

## 3. what goes wrong in overparameterized models

Consider fitting a straight line using two parameterizations:

- A minimal one with slope and intercept
- An inflated one using five constrained parameters

Both describe the same functions, but the second has directions where moving does nothing. These directions have zero Fisher information; the data cannot constrain them.

More generally, many models have likelihoods that are flat along some directions. Laplace's method treats all directions as curved, overestimating posterior contraction and over-penalizing the model.

This error scales as $\log n$, so it diverges with more data. To see this explicitly, we turn to a minimal example.

---

## 4. a minimal example: rank-deficient linear regression

Consider the linear–Gaussian model
$$
y_i = x_i^\top B\theta + \varepsilon_i, \qquad \varepsilon_i \sim \mathcal N(0, \sigma^2),
$$
with prior $\theta \sim \mathcal N(0, \tau^2 I_d)$.

Here:

- $x_i \in \mathbb{R}^p$
- $B \in \mathbb{R}^{p \times d}$
- $\theta \in \mathbb{R}^d$
- $\varepsilon_i$ is Gaussian noise
- $I_d$ is the identity matrix
- $\tau^2$ controls prior scale

If $\mathrm{rank}(B) = r < d$, only an $r$-dimensional projection of $\theta$ affects predictions.

### 4.1 exact marginal likelihood

Because the model is Gaussian, the marginal likelihood is exact. Writing $A_n = X_n B$ and $S_n = A_n^\top A_n$,
$$
\log p(D_n)
=
-\tfrac{1}{2}\Big(
n\log(2\pi)
+ n\log \sigma^2
+ \log \det(I + \alpha S_n)
+ \text{data-fit terms}
\Big),
$$
with $\alpha = \tau^2/\sigma^2$.

The spectrum of $S_n$ has:

- $r$ eigenvalues scaling as $n$
- $d-r$ eigenvalues remaining $O(1)$

### 4.2 effective dimension from eigenvalues

$$
\log \det(I + \alpha S_n)
= r \log n + O(1),
$$
so
$$
\log p(D_n)
=
\log p(D_n \mid \theta^\star)
- \frac{r}{2} \log n + O(1).
$$

### 4.3 quantifying the bic error

bic predicts a penalty of $\frac{d}{2} \log n$. the error is
$$
\Big(\frac{d}{2} - \frac{r}{2}\Big)\log n + O(1),
$$
which diverges as $n$ grows.

---

**bayesian evidence penalizes the number of directions that actually change the model, not the number of parameters used to describe it.**

---

## 5. a primer on singular learning theory (slt)

Laplace's method and BIC assume the model is *regular*: the Fisher information is full rank, the posterior concentrates at a unique point, and every parameter direction contributes curvature. Many modern models violate this.

Mixture models, neural networks, and low-rank representations all exhibit redundant parameters and flat directions in the likelihood. In such *singular* models, the set of KL minimizers
$$
\{\theta : \mathrm{KL}(p^* \| p_\theta) = 0\}
$$
can have nontrivial geometry—curves, surfaces, or more complex singularities—rather than being isolated points.

Singular learning theory (SLT), developed by Watanabe, shows that in these models the marginal likelihood is governed by the *real log canonical threshold* (RLCT) $\lambda$, a birational invariant that measures the local singularity structure:
$$
\log p(D_n) = \log p(D_n \mid \theta^*) - \lambda \log n + (m-1)\log\log n + O(1),
$$
where $m \in \mathbb{N}$ is the multiplicity. For regular models, $\lambda = d/2$ and $m = 1$, recovering BIC. For singular models, typically $\lambda < d/2$.

The key insight is that effective complexity depends on the intrinsic geometry of the likelihood—not on the number of coordinates used to describe it.

---

## 6. effective dimension and the real log canonical threshold (rlct)

SLT shows that
$$
\log p(D_n)
=
\log p(D_n \mid \theta^\star)
- \lambda \log n + O(\log \log n).
$$

$\lambda$, the RLCT, is a coordinate-free effective dimension.

In regular models, $\lambda = d/2$. In singular models, $\lambda < d/2$.

For rank-deficient regression, $\lambda = r/2$.

---

## 7. representation invariance

rlct depends only on the function class, not the parameterization. bic does not satisfy this invariance.

overcomplete representations introduce near-zero curvature directions without changing the intrinsic subspace. both parameterizations span the same subspace, but the overcomplete model introduces additional near-zero eigenvalues corresponding to redundant coordinates.

---

## 9. why this matters

Model complexity is a geometric property of the likelihood, not a bookkeeping exercise over parameters. SLT provides the correct language for understanding generalization in overparameterized models.

---

## appendix a: exact marginal likelihood and evidence slopes in linear–gaussian models

This appendix records the exact marginal likelihood calculation underlying Section 4, and explains why the leading $\log n$ term depends on intrinsic rank rather than parameter count.

We consider the linear–Gaussian regression model
$$
y_i = x_i^\top B\theta + \varepsilon_i, \quad \varepsilon_i \sim \mathcal{N}(0, \sigma^2),
$$
with prior $\theta \sim \mathcal{N}(0, \tau^2 I_d)$. Writing $X_n \in \mathbb{R}^{n \times p}$ for the design matrix and
$$
A_n := X_n B \in \mathbb{R}^{n \times d},
$$
the likelihood can be written compactly as
$$
p(y \mid \theta) = \mathcal{N}(y \mid A_n \theta, \sigma^2 I_n).
$$

Because both likelihood and prior are Gaussian, the marginal likelihood can be computed in closed form by integrating out $\theta$. A standard Gaussian completion of the square yields
$$
p(y) = \mathcal{N}(y \mid 0, \sigma^2 I_n + \tau^2 A_n A_n^\top).
$$

Taking logarithms, the log evidence is
$$
\log p(D_n) = -\frac{1}{2}\Big(n \log(2\pi) + n \log \sigma^2 + \log \det(I_d + \alpha S_n) + \text{data-fit terms}\Big),
$$
where
$$
S_n := A_n^\top A_n, \quad \alpha := \tau^2 / \sigma^2.
$$

The key object controlling the complexity penalty is the Gram matrix $S_n$.

### spectrum and intrinsic rank

Assume the rows $x_i$ are i.i.d. with nondegenerate covariance $\Sigma_x$, and that $\mathrm{rank}(B) = r < d$. Then
$$
\frac{1}{n} S_n = \frac{1}{n} B^\top X_n^\top X_n B \longrightarrow B^\top \Sigma_x B \quad \text{almost surely}.
$$

As a consequence:

- $r$ eigenvalues of $S_n$ scale linearly with $n$
- The remaining $d - r$ eigenvalues remain $O(1)$

Taking determinants,
$$
\log \det(I_d + \alpha S_n) = \sum_{j=1}^{r} \log(\alpha n \lambda_j + 1) + O(1) = r \log n + O(1).
$$

Substituting back, the leading asymptotic form of the evidence is
$$
\log p(D_n) = \log p(D_n \mid \theta^\star) - \frac{r}{2} \log n + O(1).
$$

This shows explicitly that the effective dimension of the model is $r$, not the ambient parameter count $d$. In the language of singular learning theory, the real log canonical threshold (RLCT) is
$$
\lambda = r/2.
$$

By contrast, Laplace's approximation (and BIC) always insert $d/2$ in place of $\lambda$, leading to a systematic error of
$$
\Big(\frac{d}{2} - \frac{r}{2}\Big) \log n
$$
in singular models. This is the precise sense in which BIC "over-penalizes" overparameterized representations.

---

## further reading

- kalyaan rao, "evidence slopes and effective dimension in singular linear models," preprint
- sumio watanabe, *algebraic geometry and statistical learning theory*, cambridge university press, 2009
- sumio watanabe, *mathematical theory of bayesian statistics*, chapman and hall/crc, 2018
- gideon schwarz, "estimating the dimension of a model," *the annals of statistics*, 1978

